\documentclass[a4paper]{scrartcl}
\usepackage[utf8]{inputenc} % necessary for the BiBTeX file to work with umlauts
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage[ngerman]{babel}
\usepackage{enumerate}
\usepackage{natbib} % provides a few nice styles for the bibliography
\usepackage[nottoc]{tocbibind} % makes sure the bibliography is added to the table of contents
\usepackage[fixlanguage]{babelbib}
\selectbiblanguage{german} % selects german as language for the bibliography

\newcommand{\C}{{\mathbb{C}}}
\newcommand{\R}{{\mathbb{R}}}
\newcommand{\Z}{{\mathbb{Z}}}
\newcommand{\de}{{\mathrm{d}}}
\newcommand{\ee}{{\mathrm{e}}}
\newcommand{\ii}{{\mathrm{i}}}
\newcommand{\norm}[1]{{\left\lVert#1\right\rVert}}
\newcommand{\pphi}{{\varphi}}
\newcommand{\defeq}{\overset{!}{=}}


\begin{document}

%\tableofcontents

\title{Zerlegung von univariaten Signalen in amplituden- und frequenzmodulierte IMF-Komponenten}
\author{Ralph Tandetzky}
\date{\today}
\maketitle


\section{Die Problemstellung}

Es ist ein Signal gegeben, von dem wir annehmen, dass es durch reellwertige Funktion $f:[a,b]\to\R$ auf einem Intervall $[a,b]$ beschrieben wird. 
Gesucht ist eine Zerlegung
$$ f(t) = \sum_{i=1}^N a_i(t)\cos\pphi_i(t) + r(t)\,, $$
wobei die Amplituden~$a_i$, die Phasen~$\pphi_i$ ($i=1,\dotsc,N$) und $r$ reellwertige Funktionen auf dem Intervall $[a,b]$ sind und die Restfunktion $r(t)$ minimiert werden soll. 
Die Summe ist also eine "Uberlagerung von amplituden- und frequenzmodulierten Signalen. 
Entsprechend sollen f"ur die Amplituden und Phasen mindestens folgende Bedingungen f"ur alle $i=1,\dotsc,N$ und $t\in[a,b]$ gelten:
\begin{align*}
  a_i(t) &> 0 \\
  \dot{\pphi_i}(t) &\ge 0
\end{align*}
Au"serdem sollten die Summanden $a_i(t)\cos\pphi_i(t)$ zwischen je zwei benachbarten Nullstellen genau ein Extremum besitzen, was per definitionem gerade die Eigenschaft einer IMF (intrinsic mode function) ist. 
Die Aufgabe ist es nun, die $a_i(t)$ und $\pphi_i(t)$ numerisch so zu bestimmen, dass die Restfunktion $r(t)$ minimiert wird. 

Es ist der Gegenstand dieses Dokuments, grob zu beschreiben, wie plausible Amplituden- und Phasenfunktionen durch eine geeignete Optimierung gefunden werden k"onnen. 

\section{Weitere Nebenbedingungen}

Man stellt schnell fest, dass die bisherigen Nebenbedingungen an die Amplituden und Phasen zu schwach sind, um sinnvolle Ergebnisse zu erlangen. 
Schon wenn man davon ausgeht, dass nur zwei Summanden $(N=2)$ zugelassen sind und $f$ hinreichend glatt ist, dann erlangt man eine verschwindende Restfunktion $r=0$ auf unendlich viele Weisen.
Man braucht also zus"atzliche Forderungen an die Amplituden und Phasen, um plausible L"osungen zu erhalten. 
Eine hinreichende Bedingung daf"ur, dass $a_i(t)\cos\pphi_i(t)$ eine IMF ist, lautet
\begin{align} \label{eq:condpphiiai}
  \frac14\ddot\pphi_i(t)^2 + \dot\pphi_i(t)^2\frac{\de^2}{\de t^2}\ln a_i(t) 
  \le \dot\pphi_i(t)^4\,.
\end{align}
Diese Ungleichung ist schwer handhabbar und es ist schwierig zu erkennen, was sie anschaulich bedeutet. 
Daher wurden weitere hinreichende Bedingungen untersucht. 
Setzt man
$$ \gamma_i(t) \defeq \ln a_i(t) + \ii\pphi_i(t)\,, $$
so lassen sich die Summanden vereinfacht darstellen als
$$ a_i(t)\cos\pphi_i(t) = \Re \ee^{\gamma_i(t)}. $$
Eine Amplitude und eine Phase lassen sich also in eine komplexwertige Funktion $\gamma_i(t)$ "ubersetzen und umgekehrt. 
In dieser Spache l"asst sich die bisherige hinreichende Bedingung f"ur eine IMF mit etwas Rechenaufwand weiter vereinfachen zu
\begin{align} \label{eq:imSigma}
  \lvert\ddot\gamma_i(t)\rvert \le \left(\Im\dot\gamma_i(t)\right)^2\,.
\end{align}
Wenn diese Bedingung~\eqref{eq:imSigma} erf"ullt ist, dann gilt auch~\eqref{eq:condpphiiai}, aber nicht notwendigerweise umgekehrt. 
Diese Bedingung~\eqref{eq:imSigma} ist offensichtlich erst recht erf"ullt, wenn 
\begin{align} \label{eq:absSigma}
  \lvert\ddot\gamma_i(t)\rvert \le\lvert\dot\gamma_i(t)\rvert^2\,.
\end{align}
gilt. Die Nebenbedingung $\dot{\pphi_i}(t) \ge 0$ "ubersetzt sich in $\Im\dot{\gamma_i}(t) \ge 0$. 
Unter dieser Monotonievoraussetzung gilt also zusammenfassend die Implikationskette 
$$ \text{\eqref{eq:absSigma}} \quad \Longrightarrow \quad
  \text{\eqref{eq:imSigma}} \quad \Longrightarrow \quad
  \text{\eqref{eq:condpphiiai}} \quad \Longrightarrow \quad
  \text{$a_i(t)\cos\pphi_i(t)$ ist eine IMF,} $$
wobei die Umkehrungen jeweils nicht allgemeing"ultig zutreffen. 

In der Bedingung~\eqref{eq:absSigma} wird ersichtlich, dass die zweite Ableitung von $\gamma_i(t)$ betraglich durch das Quadrat der ersten Ableitung beschr"ankt ist. 
Ginge man von einer konstanten Amplitude aus, so w"urde sich diese Forderung "aquivalent auf die Phase "ubertragen. 
Diese Forderung ist anschaulich gesehen einleuchtend. 
Damit ist also eine Nebenbedingung gefunden, die zu {\em plausiblen} L"osungen unserer Ausgangsfrage f"uhren kann. 
Gleichzeitig haben die Bedingungen~\eqref{eq:imSigma} und~\eqref{eq:absSigma} den Vorteil eine einfachere Form als~\eqref{eq:condpphiiai} zu haben, wodurch sie leichter zu "uberpr"ufen sind. 


\section{Diskretisierung}

Der n"achste Schritt auf dem Weg zu einer numerischen Optimierung besteht darin, dass wir den Raum der L"osungen soweit einschr"anken, dass er endlichdimensional ist. 
Bei verschiedenen Untersuchungen hat sich folgender Ansatz als besonders erfolgreich herausgestellt: 
Es werden als L"osungen nur st"uckweise quadratische Funktionen $\gamma_i$ zugelassen, die zudem stetig differenzierbar sind. 
Die St"utzstellen, an denen die Funktionen $\gamma_i$ nicht lokal quadratisch zu sein brauchen, sollen "aquidistant und f"ur alle Funktionen $\gamma_i$, $i=1,\ldots,N$ gleich und fest vorgegeben sein. 
Als Basis eines solchen linearen Funktionenraumes dienen dann die sogenannten B-Splines. 
Das sind gerade diejenigen st"uckweise quadratischen und stetig differenzierbaren Funktionen mit den vorgegebenen St"utzstellen und minimalem nichtleeren Tr"ager. 
Die B-Splines gehen bei "aquidistanten St"utzstellen durch Translation auseinander hervor. 
Bei diesem Ansatz stellt sich heraus, dass die Nebenbedingungen~\eqref{eq:imSigma} und~\eqref{eq:absSigma} extrem leicht f"ur alle~$t$ "uberpr"ufbar sind, wenn man die komplexwertigen Koeffizienten der B-Splines kennt. 
Darin besteht gerade der gro"se Nutzen des Ansatzes mit B-Splines.


\section{Reduktion des Parameterraumes}

Wenn die Anzahl der St"utzstellen im Zielintervall $[a,b]$ sehr gro"s ist, dann wird die numerische Optimierung unpraktikabel. 
Um das zu vermeiden, wird die Anzahl der Parameter des Optimierungsproblems weiter reduziert. 
Es seien $\gamma_{ij}$ die komplexen B-Spline-Koeffizienten von $\gamma_i(t)$ in geordneter Reihenfolge\footnote{Die B-Splines gehen durch Translation des Arguments auseinander hervor. Die Reihenfolge soll so gew"ahlt werden, dass der am weitesten links liegende B-Spline, dessen Tr"ager das Intervall $(a,b)$ noch schneidet den kleinsten Index $j$ bekommt und der am weitesten rechts liegende B-Spline mit dieser Eigenschaft den gr"o"sten Index.} bez"uglich $j$. Nun reduzieren wir die Zahl der Parameter, indem wir annehmen, dass 
$$ \gamma_{ij} = \sum_{k=1}^{d_1}b_{ik}\ee^{-\frac{(j-\mu_{ik})}{2\alpha_i^2}} + \ii\sum_{l=1}^{d_2}c_{il}\frac{1}{1+\ee^\frac{\nu_{il}-j}{\beta_i}} $$
gilt, wobei die Parameter $b_{ik}$, $c_{il}$, $\mu_{ik}$, $\nu_{il}$, $\alpha_i$ und $\beta_i$ zu bestimmen sind und $d_1$ sowie $d_2$ vorgegeben sind. 
Der Realteil setzt sich also aus einer Radialbasis und der Imagin"arteil aus einer logistischen Basis zusammen. 
Wenn $d_1$ und $d_2$ klein sind, so kann dadurch die Anzahl der Parameter drastisch reduziert werden. 


\section{Die Kostenfunktion}

F"ur eine numerische Optimierung wird eine Kostenfunktion ben"otigt, die sich zusammensetzt aus
\begin{itemize}
 \item einer Norm f"ur die Restfunktion $r(t) = f(t) - \sum_{i=1}^N a_i(t)\cos\pphi_i(t)$ und
 \item Straftermen f"ur die Verletzung von Nebenbedingungen. 
\end{itemize}
F"ur die Norm der Restfunktion $r(t)$ k"onnte man $L_p$-Normen oder Sobolev-Normen verwenden. 
Da wir jedoch davon ausgehen k"onnen, dass uns das Signal~$f(t)$ in Form von Samples vorliegt und nicht als Funktion auf dem kontinuierlichen Intervall $[a,b]$, bietet es sich an, die $\ell_p$-Norm der Werte an den Samplestellen als Norm zugrunde zu legen. 
Die Norm besitzt dann also die Form
\begin{align} \label{eq:lpnormr}
\norm{r}_{\ell_p} \defeq \left(\,\sum_{t}\lvert r(t)\rvert^p\right)^{1/p},
\end{align}
wobei $t$ "uber alle Samples l"auft. 
Dies hat den Vorteil, dass das Signal $f$ nur an den Samplestellen ben"otigt wird. 
Bei $L_p$-Normen oder Sobolev-Normen hingegen w"are eine Interpolation oder dergleichen notwendig und es w"urde wahrscheinlich eine rechenintensive numerische Integration gebraucht werden. 

In der Implementierung des Algorithmus richten sich die Strafterme, die bei der Verletzung von Nebenbedingungen verwendet werden, prim"ar nach der Gesamtbreite aller Intervalle, auf dem die Nebenbedingungen verletzt sind. 
Au"serdem geht in die Strafterme ein, wie schwerwiegend die entsprechenden Ungleichungen der Nebenbedingung verletzt werden. 
Es ist anzumerken, dass der Strafterm unstetig in den Parametern ist, da die Verletzung der Nebenbedingungen schon auf einem kleinen Intervall einen gen"ugend gro"sen Strafterm liefern soll, der sicherstellt dass die Nebenbedingungen nach einer Optimierung auch wirklich erf"ullt sind. 


\section{Die globale Optimierung}

Als Optimierungsverfahren wurde eine Implementierung des Differential Evolution Algorithmus~\citep*{StPr1997} verwendet. 
Dieser Algorithmus erh"alt als Eingabe die Kostenfunktion und eine Menge (ein Schwarm) von L"osungskandidaten. 
Dieser Schwarm wird im Laufe des Algorithmus schrittweise verbessert, indem L"osungskandidaten mit hohen Kosten durch neue L"osungskandidaten mit niedrigeren Kosten ersetzt werden. 
Wenn ein vorgegebenes Abbruchkriterium erf"ullt ist, wird der beste L"osungskandidat als Ergebnis zur"uckgeliefert. 

F"ur den Erfolg des Verfahrens sind gute L"osungskandidaten entscheidend. 
Bei umfangreichen Tests hat sich ergeben, dass es besonders wichtig ist, mit einer guten Approximation der Phasenfunktion zu beginnen. 
Der Differential Evolution Algorithmus hat dann keine Probleme die Amplitude entsprechend anzupassen. 
Eine geeignete Anfangsapproximation der Phase ergibt sich, indem man nur die Nullstellen und die Vorzeichen der Funktion betrachtet und die Phase so einrichtet, dass die Nullstellen "ubereinstimmen. 
Als Anfangsapproximation der Amplitude reicht es aus, eine Konstante anzunehmen. 
Der Schwarm wird dann erzeugt, indem man L"osungskandidaten w"ahlt, die normalverteilt sind, so dass der Erwartungswert die gerade besprochene Anfangsapproximation ist. 


\section{Vorverarbeitungsschritte}

Bei der Berechnung der Anfangsapproximation spielen die Nullstellen eine erhebliche Rolle. 
Ist auf den Samples ein gro"ses Rauschen, so kann es schnell dazu kommen, dass zu viele Nullstellen gez"ahlt werden und somit die Anfangsapproximation ungeeignet wird. 
Um dem vorzubeugen, kann man das Signal-Rausch-Verh"altnis verbessern, indem man einen Tiefpassfilter auf die Samples anwendet. 
Au"serdem sollte man daf"ur sorgen dass der Mittelwert der Samples Null ist. 
Es hat sich herausgestellt, dass das Verfahren besonders gut funktioniert, wenn man das erste und zweite Moment der Samples entfernt, indem die entsprechende quadratische Funktion von den Samples abzieht.
Damit wird gleichzeitig sichergestellt, dass die Zielfunktion mindestens drei Nullstellen besitzt. 


\section{Iterative Berechnung der IMF-Komponenten}

Der Parameterraum ist im Falle von mehreren IMF-Komponenten immernoch sehr gro"s. 
Die Rechenzeit w"achst sehr stark mit der Anzahl der zu optimierenden Parameter. 
Daher wird in der derzeitigen Implementierung immer nur eine IMF-Komponente zu einer Zeit optimiert und nicht alle gleichzeitig. 
Wenn eine IMF-Komponente optimiert wurde, wird anschlie"send mit der n"achsten Komponente fortgefahren. 
Dies setzt sich fort, bis der Restfehler sich auf Rauschniveau befindet und abgebrochen wird. 

Noch weitaus bessere Ergebnisse erlangt man, wenn nach der Optimierung der zweiten IMF die erste IMF-Komponente auf Grundlage der zweiten nachgebessert wird. 
Ebenso kann man nach der Berechnung der dritten IMF-Komponente die beiden vorherigen nochmals nachbessern.
Es hat sich gezeigt, dass dieses Verfahren zu einer erheblichen Verringerung des Restfehlers f"uhrt. 
Die iterative Optimierung der IMF-Komponenten k"onnte also beispielsweise in der Reihenfolge 
$$ 1 
\ \ \longrightarrow \ \ 2
\rightarrow 1 
\ \ \longrightarrow \ \ 3
\rightarrow 2
\rightarrow 1 
\ \ \longrightarrow \ \ 4
\rightarrow 3
\rightarrow 2
\rightarrow 1 $$
ablaufen. 

%- Konkrete Ergebnisse in Testl"aufen
%- kurze Beschreibung der entstandenen Software


\bibliographystyle{plainnat}
\bibliography{decompose_imf}


\end{document}
