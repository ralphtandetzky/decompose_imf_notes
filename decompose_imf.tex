\documentclass[a4paper]{scrartcl}
\usepackage{amsmath,amsfonts,amsthm}
\usepackage{ngerman}
\usepackage{enumerate}

\newcommand{\R}{{\mathbb{R}}}
\newcommand{\de}{{\mathrm{d}}}
\newcommand{\ii}{{\mathrm{i}}}
\newcommand{\norm}[1]{{\left\lVert#1\right\rVert}}
\newcommand{\pphi}{{\varphi}}
\newcommand{\defeq}{\overset{!}{=}}

\begin{document}

\tableofcontents

\section{Aufgabenstellung}

Sei $f:[a,b]\to\R$ eine hinreichend glatte reellwertige Funktion auf einem Intervall $[a,b]$. Gesucht ist eine Zerlegung
$$ f(t) = \sum_{i=1}^N a_i(t)\cos\pphi_i(t) + r(t)\,, $$
wobei $a_i,\pphi_i$ f"ur $i=1,\dotsc,N$ und $r$ reellwertige Funktionen auf dem Intervall $[a,b]$ sind und die Restfunktion $r(t)$ minimiert werden soll. F"ur die Funktionen $a_i$ und $\pphi_i$ f"ur $i=1,\dotsc,N$ sollen dabei mindestens folgende Bedingungen f"ur alle $t$ gelten:
\begin{align*}
  a_i(t) &> 0 \\
  \dot{\pphi_i}(t) &\ge 0
\end{align*}
Au"serdem sollten die Summanden $a_i(t)\cos\pphi_i(t)$ zwischen je zwei benachbarten Nullstellen genau ein Extremum besitzen, so dass die Eigenschaft einer IMF (intrinsic mode function) erf"ullt ist. Die Aufgabe ist es, die $a_i(t)$ und $\pphi_i(t)$ numerisch so zu bestimmen, dass die Restfunktion $r(t)$ m"oglichst klein wird. Dazu ist Folgendes notwendig:
\begin{enumerate}
  \item Neben den obigen Forderungen an die Funktionen $a_i$ und $\pphi_i$, sollten weitere praktikable Bedingungen festgelegt werden, damit eine plausible L"osung des Problems gefunden wird. Dies wird im Abschnitt~\ref{sec:moreconds} behandelt. 
  \item Die Funktionen $a_i$ und $\pphi_i$ m"ussen geeignet diskretisiert werden, etwa indem man sie in Linearkombinationen einfacher Funktionen zerlegt. Darauf wird in Abschnitt~\ref{sec:discretize} eingegangen.
  \item F"ur die Restfunktion muss eine geeignete Norm festgelegt werden, damit "uberhaupt klar ist, was es hei"st, den Fehler zu minimieren. Hierauf wird in Abschnitt~\ref{sec:norms4r} eingegangen.
  \item Man braucht ein geeignetes Optimierungsverfahren, um die L"osungen schrittweise zu verbessern. Denkbar sind etwa Gradientenverfahren. Dabei ist zu beachten, dass die Nebenbedingungen stets eingehalten werden m"ussen. Dies ist sicherlich erst dann sinnvoll, wenn man sich sehr nah an einem guten lokalen Optimum befindet. Verschiedene Ans"atze werden in Abschnitt~\ref{sec:gradients} besprochen.
  \item Bei dem Optimierungsproblem wird es im Allgemeinen sehr viele lokale Extremstellen geben. Es sollte also eine gute erste N"aherung f"ur die $a_i$ und $\pphi_i$ gefunden werden. Dies wird in Abschnitt~\ref{sec:initialval} beschrieben. 
  \item Au"serdem sollte im Algorithmus vermieden werden, dass man sich in einem schlechten lokalen Extrempunkt verf"angt und nicht mehr davon wegkommt. Abschnitt~\ref{sec:jumps} geht darauf ein, wie das gelingt.
\end{enumerate}


\section{Weitere Nebenbedingungen} \label{sec:moreconds}

Die Bedingung an die $a_i(t)$ ist sehr schwach. Prinzipiell k"onnte man $\pphi_1(t):=0$ und $\pphi_2(t):=\pi$ setzen und dann die gesamte Approximation mit geeigneten $a_1$ und $a_2$ ausf"uhren. Dadurch wird allerdings im Allgemeinen die Bedingung verletzt, dass zwischen zwei benachbarten Nullstellen nur ein Extrempunkt liegen darf verletzt. Wir brauchen also eine geeignete Bedingung, die sich leicht in eine analytische Form bringen l"asst, so dass die Extrempunkteigenschaft sichergestellt ist. 

Wir untersuchen zun"achst die Nullstellen der ersten Ableitung eines Summanden $a_i(t)\cos\pphi_i(t)$. Um uns etwas Schreibarbeit zu sparen, verzichten wir darauf, den Index $i$ jedesmal zu schreiben, solange er keine Rolle spielt. Die Ableitung von $a(t)\cos\pphi(t)$ ist
$$ \dot a(t)\cos\pphi(t)-a(t)\dot\pphi(t)\sin\pphi(t)\,. $$
Zwischen zwei Nullstellen k"onnen wir diesen Ausdruck problemlos durch $a(t)\cos\pphi(t)$ teilen und erhalten so
$$ \frac{\dot a(t)}{a(t)}
-\dot\pphi(t)\tan\pphi(t)\,. $$
Dieser Ausdruck l"auft gegen $+\infty$, wenn man sich einer Nullstelle von $a(t)\cos\pphi(t)$ von rechts n"ahert und gegen $-\infty$, wenn man sich von links einer Nullstelle n"ahert. Aus dem Zwischenwertsatz folgt also, dass dieser Ausdruck mindestens eine Nullstelle zwischen zwei benachbarten Nullstellen von $a(t)\cos\pphi(t)$ besitzt. Wir wollen nun Voraussetzungen herleiten, unter denen dieser Ausdruck zwischen den Nullstellen streng monoton f"allt. Aus der Monotonie folgt dann die Eindeutigkeit der Nullstelle und somit dass es nur eine Extremstelle von $a(t)\cos\pphi(t)$ zwischen benachbarten Nullstellen gibt. Um Voraussetzungen f"ur die Monotonie zu erlangen, leiten wir den Ausdruck zun"achst ab und bekommen
$$ \frac{\de}{\de t}\frac{\dot a(t)}{a(t)}
-\ddot\pphi(t)\tan\pphi(t)
-\frac{\dot\pphi(t)^2}{\cos^2\pphi(t)}\,. $$
Multipliziert mit $\cos^2\pphi(t)$ ergibt das
$$ \cos^2\pphi(t)\frac{\de}{\de t}\frac{\dot a(t)}{a(t)}
-\frac12\ddot\pphi(t)\sin 2\pphi(t)
-\dot\pphi(t)^2\,. $$
Wir bleibt also eine Bedingung herzuleiten, unter der dieser Ausdruck nicht positiv werden kann. Dies ist "aquivalent zu
\begin{align} \label{eq:AxByC}
  Ax + By \le C\,,
\end{align}
wobei wir 
$$ 
A:=\frac{d^2}{dt^2}\ln a(t)\,, \quad
B:=\ddot\pphi(t)\,, \quad
C:=\dot\pphi(t)^2\,, \quad
x:=\cos^2\pphi(t)\,, \quad
y:=-\frac12\sin2\pphi(t) $$
setzen. Die Gr"o"sen $x$ und $y$ gehorchen der Nebenbedingung
$$ \left(x-\frac12\right)^2 + y^2 = \frac14\,. $$
Wir nehmen jetzt $A$ und $B$ als feste Gr"o"sen an, und wollen das Maximum des Ausdrucks $Ax+By$ unter dieser Nebenbedingung berechnen. Als Lagrange-Funktion f"ur dieses Problem haben wir
$$ \Lambda(x,y,\lambda) 
:= Ax + By + \left( \left(x-\frac12\right)^2 + y^2 - \frac14 \right) \lambda \,. $$
Leiten wir die Langrange-Funktion nach $x$ und $y$ ab, so erhalten wir
\begin{alignat*}{4}
  \frac{\partial\Lambda}{\partial x}(x,y,\lambda) &= A + (2x-1)\lambda &&\defeq 0 
  &\qquad&\Longrightarrow &\qquad& \lambda = -\frac{A}{2x-1}\,, \\
  \frac{\partial\Lambda}{\partial y}(x,y,\lambda) &= B +  2y   \lambda &&\defeq 0 
  && \Longrightarrow && \lambda = -\frac{B}{2y}\,.
\end{alignat*}
Setzen wir die erhaltenen Gleichungen f"ur $\lambda$ gleich, so bekommen wir 
$$ (2x-1)B = 2yA \qquad \Longrightarrow \qquad
x-\frac12 = \frac AB y\,. $$
Setzen wir dies in die Nebenbedingung ein, so entsteht
$$ \frac{A^2}{B^2}y^2 + y^2 = \frac{A^2+B^2}{B^2}y^2 
= \frac14 \qquad \Longrightarrow \qquad
y=\pm\frac B2\sqrt{\frac{1}{A^2+B^2}}\,. $$
Damit k"onnen wir auch $x$ ermitteln:
$$ x = \pm\frac A2\sqrt{\frac{1}{A^2+B^2}} + \frac12 $$
Unter der obigen Nebenbedingung nimmt $Ax+By$ somit das Maximum
$$ \frac12 \sqrt{A^2+B^2}+\frac A2 $$
an. Also ist die Ungleichung~\eqref{eq:AxByC} erf"ullt, wenn 
\begin{align} \label{eq:intermediateABC}
  \frac12\sqrt{A^2+B^2}+\frac A2 \le C 
\end{align}
gilt. Wir bringen $A/2$ auf die andere Seite, quadrieren und ziehen dann $A^2/4$ auf beiden Seite ab. Es entsteht die "aquivalente Ungleichung
$$ \frac14 B^2 \le C^2 - AC. $$
Wir bringen den Ausdruck $AC$ auf die andere Seite und erhalten nach Einsetzen von $A$, $B$ und $C$ die Ungleichgung
\begin{align} \label{eq:condpphiiai}
  \frac14\ddot\pphi(t)^2 + \dot\pphi(t)^2\frac{d^2}{dt^2}\ln a(t) 
  \le \dot\pphi(t)^4\,.
\end{align}
Wenn also diese Ungleichung gilt, dann ist sichergestellt, dass zwischen zwei Nullstellen von $a(t)\cos\pphi(t)$ nicht mehr als eine lokale Extremstelle existiert. Es ist bemerkenswert, dass diese Ungleichung nicht von der momentanen Phase $\pphi(t)$ abh"angt, sondern nur von der Winkelgeschwindigkeit, der Winkelbeschleunigung und der zweiten logarithmischen Ableitung von $a(t)$. Es folgt also, dass $a(t)\cos\pphi(t)$ auch dann noch eine IMF ist, wenn man zur Phase eine konstante Funktion hinzuaddiert. Dasselbe Argument gilt f"ur $a(t)$: Gilt die obige Bedingung, dann bleibt $a(t)\cos\pphi(t)$ auch dann noch eine IMF, wenn man  $a(t)$ mit einer beliebigen Funktion der Form $e^{ct+d}$ multipliziert, denn die zweite logarithmische Ableitung von $a(t)$ bleibt unver"andert. 

Die Bedingung~\eqref{eq:condpphiiai} sieht sehr kompliziert aus. "Au"serlich erscheint sie etwas einfacher, wenn man
$$ \alpha(t):=\ln a(t) $$
einf"uhrt. Dann kann man die zweite logarithmische Ableitung von $a(t)$ schreiben als $\frac{d^2}{dt^2}\ln a(t) = \ddot\alpha(t)$. F"uhrt man dann noch die komplexwertige Funktion
$$ \sigma(t) := \alpha(t) + \ii\pphi(t) $$
ein, so haben wir die kompaktere Darstellung
$$ a(t)\cos\pphi(t) = \Re e^{\sigma(t)}\,. $$
Die Ungleichung~\eqref{eq:intermediateABC} ist erf"ullt, wenn 
$$ \sqrt{A^2 + B^2} \le C $$
gilt. Dies ist "aquivalent zu der Formulierung
\begin{align} \label{eq:imSigma}
  \lvert\ddot\sigma(t)\rvert \le \left(\Im\dot\sigma(t)\right)^2\,.
\end{align}
Man kann alle Aussagen unserer Problemstellung statt mit $a(t)$ und $\pphi(t)$ ausgehend von $\sigma(t)$ beschreiben. Der Vorteil dieser Sichtweise ist, dass die Nebenbedingungen $a(t)>0$ wegen $a(t)=|e^{\sigma(t)}|$ automatisch erf"ullt ist. Es bleibt nur noch 
$$ \Im\dot\sigma(t)\ge0 $$ 
zu beachten und wir haben damit eine kleine Menge von sehr einfachen Nebenbedingungen. Eine noch st"arkere, aber auch handhabbarere Bedingung ist 
\begin{align} \label{eq:absSigma}
  \lvert\ddot\sigma(t)\rvert \le\lvert\dot\sigma(t)\rvert^2\,.
\end{align}
Zusammenfassend haben wir also die Implikationskette 
$$ \text{\eqref{eq:absSigma}} \quad \Longrightarrow \quad
  \text{\eqref{eq:imSigma}} \quad \Longrightarrow \quad
  \text{\eqref{eq:condpphiiai}} \quad \Longrightarrow \quad
  \text{$a(t)\cos\pphi(t)$ ist eine IMF,} $$
wobei die Umkehrungen jeweils nicht allgemeing"ultig zutreffen. 


\section{Diskretisierung der Amplituden- und Phasenfunktionen} \label{sec:discretize}

\subsection{Na"iver Ansatz}

Um die Funktionen $a_i$ und $\pphi_i$ numerisch ann"ahern zu k"onnen, muss zun"achst eine Diskretiesierung vorgenommen werden. Daf"ur bietet es sich an, die Funktionen als Linearkombinationen darzustellen:
\begin{align*}
      a_i(t) &= \sum_{m=0}^{M_a    }     a_{im}    b_m(t)\,, \\ 
  \pphi_i(t) &= \sum_{m=0}^{M_\pphi} \pphi_{im} \psi_m(t)\,.
\end{align*}
Geht man von der alternativen Formulierung mit $\sigma_i(t)$ statt $a_i(t)$ und $\pphi_i(t)$ aus, so k"onnte man auch den Ansatz
\begin{align*}
  \sigma_i(t) &= \sum_{m=0}^M \sigma_{im}g_m(t)
\end{align*}
mit komplexwertigen Koeffizienten $\sigma_{im}$ w"ahlen. Die Basisfunktionen $g_m(t)$ k"onnten auch komplexwertig sein. Jedoch ist das nicht notwendig und unter Umst"anden auch gar nicht erw"unscht. 

Als Basisfunktionen w"aren beispielsweise Polynome oder trigonometrische Polynome denkbar. Eine konstante Funktion und ein Polynom erster Ordnung erscheinen sehr sinnvoll. Als weitere Basisfunktionen k"onnten auch solche dienen, deren zweite Ableitung Wavelets mit bestimmten Eigenschaften sind. Wavelets haben den Vorteil, dass sie gut lokalisiert sind. Wenn bei den Wavelets au"serdem die nullten und ersten Momente verschwinden, dann ist auch die entsprechende Basisfunktion lokalisiert. Bei der numerischen Optimierung k"onnte auch die Orthogonalit"at von Wavelets eine entscheidende Rolle spielen. 


\subsection{Erster angepasster Ansatz}

Wir wollen nun einen Ansatz behandeln, der speziell auf unser Problem zugeschnitten ist. Der Ansatz im vorigen Abschnitt hat den Nachteil, dass es aufwendig sein kann, die Nebenbedingungen w"ahrend des Optimierens zu "uberpr"ufen. In diesem Abschnitt werden wir einen Ansatz besprechen, bei dem die Nebenbedingungen trivial "uberpr"ufbar sind. Dazu definieren wir
\begin{align} \label{eq:def_theta} 
\theta(t) := \frac{\ddot\sigma(t)}{\dot\sigma(t)^2}. 
\end{align}
Die Nebenbedingung~\eqref{eq:absSigma} ist also genau dann erf"ullt, wenn 
$$ |\theta(t)| \le 1. $$
Jetzt drehen wir den Spie"s um: Wenn $\theta(t)$ bekannt ist, dann ist $\sigma(t)$ durch Gleichung~\eqref{eq:def_theta} bis auf zwei Integrationskonstanten eindeutig festgelegt. Es gilt dann n"amlich 
$$ -\frac{1}{\dot\sigma(t)} = \int\theta(t)\,\de t 
\qquad \Longrightarrow \qquad 
\sigma(t) = -\int \frac{1}{\int\theta(t)\,\de t}\,\de t $$
Nun ist die Idee, dass man $\theta(t)$ und die Integrationskonstanten optimiert. Der Vorteil ist, dass die Nebenbedingung sehr einfach zu verifizieren ist. Als Ansatz nehmen wir an, dass $\theta(t)$ st"uckweise konstant ist. Auf einem Intervall, wo $\theta(t)=c_0$ konstant ist, gilt dann 
$$ \sigma(t) = -\int \frac{\de t}{c_0t + c_1}
=-\frac{1}{c_0}\ln(c_0t+c_1) + c_2 $$
f"ur geeignete Werte von $c_0$, $c_1$ und $c_2$. Mit 
$$ A := -\frac{1}{c_0}\,, \quad
B:= -\frac{c_1}{c_0}\,, \quad
C:= c_2 + A\ln c_0 $$
gilt 
$$ \sigma(t) = A\ln(t-B) + C\,. $$
Die Nebenbedingungen 
$$ |\theta(t)| \le 1 \qquad \text{und} \qquad
\Im\dot\sigma(t)\ge 0 $$
k"onnen umformuliert werden zu
$$ |A|\ge 1 \qquad \text{und}\qquad
\Im\left[A(t-B)^*\right]\ge 0\,, $$
wobei $(t-B)^*$ der konjugiert komplexe Wert von $t-B$ sein soll. Die Bedingung links kann trivial "uberpr"uft werden, die Bedingung rechts braucht lediglich an den Endpunkten der Intervalle "uberpr"uft zu werden, auf denen $\theta(t)$ konstant ist. Wenn die Bedingung dort zutrifft, dann auch im Inneren des Intervalls. 


\subsection{Lokale Ver"anderungen beim ersten Ansatz}

Der Ansatz aus dem vorangegangenen Unterabschnitt erleichtert die "Uberpr"ufung der Nebenbedingungen. Allerdings muss die Funktion $\theta(t)$ optimiert werden und lokale "Anderungen bei dieser Funktion k"onnen globale Auswirkungen auf die Funktion $\sigma(t)$ nach sich ziehen. Wir wollen nun untersuchen, wie man die Funktion $\theta(t)$ lokal ver"andern kann, damit man bei $\sigma(t)$ ebenfalls nur lokale Ver"anderungen vorfindet. 

Man kann dies etwa damit vergleichen, dass man eine st"uckweise konstante Funktion kennt und wissen m"ochte, wie man sie ver"andern kann, so dass nach zweifachem Integrieren eine Funktion herauskommt, die nur lokale "Anderungen aufweist. Bei "aquidistanten Intervallen braucht man in diesem Fall drei benachbarte Intervalle mit den Wichtungen $1$, $-2$ und $1$ in dieser Reihenfolge, damit sich die zweimal integrierte Funktion nur dort und nirgends sonst "andert. 

In unserem Fall versuchen wir es ebenfalls mit drei Intervallen $[t_0,t_1]$, $[t_1,t_2]$ und $[t_2,t_3]$. Da $\sigma(t)$ einmal stetig differenzierbar ist, stimmen die Werte der Funktion und deren Ableitungen an den Intervallgrenzen "uberein. Da sich $\sigma(t)$ au"serhalb der Intervalle nicht ver"andern darf, k"onnen wir davon ausgehen, dass die Werte von $\sigma(t)$ und deren Ableitung an den Stellen $t_0$ und $t_3$ fest und bekannt sind. Die Parameter $A$, $B$ und $C$ aus dem vorangegangen Abschnitt unterscheiden sich auf den drei Intervallen. Wir bezeichnen diese Parameter auf $[t_{i-1},t_i]$ mit $A_i$, $B_i$ und $C_i$ f"ur $i=1,\dotsc,3$. Au"serdem schreiben wir der K"urze halber $\sigma_i:=\sigma(t_i)$ und $\dot\sigma_i:=\dot\sigma(t_i)$ f"ur $i=0,\dotsc,3$. Mit diesen Bezeichnungen bekommen wir folgendes Gleichungssystem:
\begin{align}
  \sigma_0       &= A_1\ln(t_0-B_1)+C_1 \label{eq:local1} \\
  \dot\sigma_0 &= \frac{A_1}{t_0-B_1} \label{eq:local2} \\
  \sigma_1       &= A_1\ln(t_1-B_1)+C_1 \label{eq:local3} \\
  \dot\sigma_1 &= \frac{A_1}{t_1-B_1} \label{eq:local4} \displaybreak[0] \\
  \sigma_1       &= A_2\ln(t_1-B_2)+C_2 \label{eq:local5} \\
  \dot\sigma_1 &= \frac{A_2}{t_1-B_2} \label{eq:local6} \\
  \sigma_2       &= A_2\ln(t_2-B_2)+C_2 \label{eq:local7} \\
  \dot\sigma_2 &= \frac{A_2}{t_2-B_2} \label{eq:local8} \displaybreak[0] \\
  \sigma_2       &= A_3\ln(t_2-B_3)+C_3 \label{eq:local9} \\
  \dot\sigma_2 &= \frac{A_3}{t_2-B_3} \label{eq:local10} \\
  \sigma_3       &= A_3\ln(t_3-B_3)+C_3 \label{eq:local11} \\
  \dot\sigma_3 &= \frac{A_3}{t_3-B_3} \label{eq:local12}
\end{align}
Damit haben wir 12 Gleichungen und 13 Unbekannte, n"amlich $A_1$, $B_1$, $C_1$, $A_2$, $B_2$, $C_2$, $A_3$, $B_3$, $C_3$, $\sigma_1$, $\dot\sigma_1$, $\sigma_2$ und $\dot\sigma_2$. Alle anderen Gr"o"sen $\sigma_0$, $\dot\sigma_0$, $\sigma_3$, $\dot\sigma_3$, $t_0$, $t_1$, $t_2$ und $t_3$ setzen wir als bekannt voraus. Somit verbleibt also ein Freiheitsgrad. Man kann sich also zum Beispiel den Wert $A_2$ vorgeben und dann alle restlichen Parameter daraus berechnen. Wir wollen diese Rechnung so weit wie m"oglich analytisch durchf"uhren. Zun"achst stellen wir~\eqref{eq:local2} nach $B_1$ um und erhalten so 
\begin{align} \label{eq:B1}
  B_1=t_0-\frac{A_1}{\dot\sigma_0}\,.
\end{align}
Setzen wir dies in~\eqref{eq:local1} ein und stellen nach $C_1$ um, so erhalten wir
\begin{align} \label{eq:C1}
  C_1=\sigma_0-A_1\ln\frac{A_1}{\dot\sigma_0}\,.
\end{align}
Setzen wir den Ausdruck f"ur $B_1$ in~\eqref{eq:local4} ein, so erhalten wir 
$$ \dot\sigma_1 = \frac{A_1}{t_1-t_0+\frac{A_1}{\dot\sigma_0}}\,. $$
Setzen wir die Ausdr"ucke f"ur $B_1$ und $C_1$ in~\eqref{eq:local3} ein, so entsteht
$$ \sigma_1 = A_1\ln\left(t_1-t_0+\frac{A_1}{\dot\sigma_0}\right)+\sigma_0-A_1\ln\frac{A_1}{\dot\sigma_0}\,. $$
Wir setzen die erhaltenen Ausdr"ucke in~\eqref{eq:local6} ein, und bekommen
\begin{align} \label{eq:B2}
  B_2 = t_1-\frac{A_2}{A_1}(t_1-t_0)-\frac{A_2}{\dot\sigma_0}\,.
\end{align}
Ebenso verfahren wir mit Gleichung~\eqref{eq:local5}. Es entsteht
\begin{equation} \label{eq:C2}
\begin{split}
  C_2 
  &= A_1\ln\left(t_1-t_0+\frac{A_1}{\dot\sigma_0}\right)+\sigma_0-A_1\ln\frac{A_1}{\dot\sigma_0}-A_2\ln\left(\frac{A_2}{A_1}(t_1-t_0)+\frac{A_2}{\dot\sigma_0}\right) \\
  &= (A_1-A_2)\ln\left(t_1-t_0+\frac{A_1}{\dot\sigma_0}\right)+\sigma_0-A_1\ln\frac{A_1}{\dot\sigma_0}-A_2\ln\frac{A_2}{A_1}\,.
\end{split}
\end{equation}
Das Gleichungssystem bestehend aus den Gleichungen \eqref{eq:local1} bis~\eqref{eq:local12} ist symmetrisch. Vertauscht man die folgenden Gr"o"sen, so erh"alt man dasselbe Gleichungssystem, allerdings in einer anderen Reihenfolge der Gleichungen: $A_1\leftrightarrow A_3$, $B_1\leftrightarrow B_3$, $C_1\leftrightarrow C_3$, $\sigma_0\leftrightarrow \sigma_3$, $\sigma_1\leftrightarrow \sigma_2$, $\dot\sigma_0\leftrightarrow \dot\sigma_3$, $\dot\sigma_1\leftrightarrow \dot\sigma_2$, $t_0\leftrightarrow t_3$ und $t_1\leftrightarrow t_2$. Also gelten die Gleichungen \eqref{eq:B2} und~\eqref{eq:C2} auch noch nach dieser Vertauschung der Gr"o"sen. Sie lauten dann 
\begin{gather}
  \label{eq:B2symm}
  B_2 = t_2-\frac{A_2}{A_3}(t_2-t_3)-\frac{A_2}{\dot\sigma_3}\,, \\
  \label{eq:C2symm}
  C_2 = (A_3-A_2)\ln\left(t_2-t_3+\frac{A_3}{\dot\sigma_3}\right)+\sigma_3-A_3\ln\frac{A_3}{\dot\sigma_3}-A_2\ln\frac{A_2}{A_3}\,.
\end{gather}
Wir teilen nun die Gleichungen \eqref{eq:B2} und~\eqref{eq:B2symm} durch~$A_2$ und setzen dann beide Seiten gleich. Es entsteht dadurch
$$ \frac{t_0-t_1}{A_1}+\frac{t_1-t_2}{A_2}+\frac{t_2-t_3}{A_3} 
  = \frac{1}{\dot\sigma_0}-\frac{1}{\dot\sigma_3}\,. $$
Setzt man die Gleichungen \eqref{eq:C2} und~\eqref{eq:C2symm} gleich und formt geschickt um, so erh"alt man
$$ (A_1-A_2)\ln\left(\frac{\dot\sigma_0}{A_1}(t_1-t_0)+1\right)+\sigma_0
  = (A_3-A_2)\ln\left(\frac{\dot\sigma_3}{A_3}(t_2-t_3)+1\right)+\sigma_3\,. $$
Durch diese beiden Gleichungen sind direkte Beziehungen zwischen den Unbekannten $A_1$ und~$A_3$ hergestellt. Alle anderen Gr"o"sen in den beiden Gleichungen sind bekannt. Diese Gleichungen kann man numerisch nach $A_1$ und~$A_3$ aufl"osen. Sind $A_1$ und~$A_3$ bekannt, so kann man damit die Gr"o"sen $B_1$, $C_1$, $B_2$, $C_2$, $B_3$ und $C_3$ durch die Beziehungen \eqref{eq:B1}, \eqref{eq:C1}, \eqref{eq:B2}, \eqref{eq:C2} und 
\begin{align*}
  B_3 &= t_3-\frac{A_3}{\dot\sigma_3}\,, \\
  C_3 &= \sigma_3-A_3\ln\frac{A_3}{\dot\sigma_3}
\end{align*}
berechnen. Die beiden letzten Gleichungen entstehen durch Austauschen der Variablen bei den Gleichungen \eqref{eq:B1} und~\eqref{eq:C1}. Alternativ erh"alt man diese Gleichungen auch, indem man zun"achst Gleichung~\eqref{eq:local12} nach~$B_3$ umstellt und dann in~\eqref{eq:local11} einsetzt diese Gleichung und nach~$C_3$ umstellt. 


\subsection{Bewertung des ersten Ansatzes}

Der gro"se Vorteil des ersten Ansatzes ist, dass die Nebenbedingunen trivial "uberpr"ufbar sind. Es ist m"oglich, $\theta(t)$ so lokal abzu"andern, dass sich $\sigma(t)$ ebenfalls nur lokal "andert. Allerdings muss man, daf"ur ein Gleichungssystem l"osen, welches vermutlich nur numerisch l"osbar ist. Dieser Schritt des numerischen L"osens ist bei einer einfachen Implementierung wahrscheinlich so aufwendig, dass der Vorteil, dass die Nebenbedingungen trivial "uberpr"uft werden k"onnen sich aufhebt. 

Im n"achsten Abschnitt versuchen wir einen weiteren Ansatz zu erarbeiten, der wahrscheinlich beide Probleme l"ost.


\subsection{Zweiter angepasster Ansatz} \label{sec:2ndansatz}

Wir wollen zeigen, dass die Nebenbedingungen 
$$ \lvert\ddot\sigma(t)\rvert \le (\Im\dot\sigma(t))^2 
\qquad \text{und} \qquad
\Im\dot\sigma(t)\ge0 $$
f"ur st"uckweise quadratische, stetig differenzierbare Funktionen besonders leicht "uberpr"uft werden k"onnen. Auf einem Intervall~$I$, auf dem $\sigma(t)$ quadratisch ist, ist $\Im\dot\sigma(t)$ monoton. Also gilt die zweite Nebenbedigung $\Im\dot\sigma(t)\ge0$ genau dann, wenn sie an beiden Endpunkten dieses Intervalls erf"ullt ist. Wenn $\Im\dot\sigma(t)$ monoton und nicht-negativ ist, dann gilt dasselbe f"ur das Quadrat davon, welches auf der rechten Seite der ersten Nebenbedingung steht. Auf der linken Seite dieser Ungleichgung steht eine Konstante. Daher ist auch die erste Nebenbedingung automatisch erf"ullt, wenn sie an den Endpunkten gilt und die zweite Nebenbedingung zutrifft. Kurz zusammengefasst sind die beiden Nebenbedingungen genau dann erf"ullt, wenn sie an den Endpunkten der Intervalle erf"ullt ist, auf denen $\sigma(t)$ quadratisch ist.

Ganz konkret sei $[t_0,t_1]$ ein Intervall, auf dem $\sigma(t)$ quadratisch ist, und zwar
$$ \sigma(t) = at^2 + bt + c $$
Dann erf"ullt $\sigma(t)$ die beiden Nebenbedingungen auf dem Intervall genau dann, wenn 
$$ 2\,\lvert a\rvert \le (2\,\Im a\,t_j + \Im b)^2
\qquad \text{und} \qquad
2\,\Im a\,t_j + \Im b\ge 0 $$
f"ur $j=0,1$. 


\subsection{Lokale Ver"anderungen beim zweiten Ansatz}

Ebenso wie beim ersten Ansatz stellt sich die Frage, wie man st"uckweise quadratische $\sigma(t)$ lokal ab"andern kann ohne dabei die stetige Differenzierbarkeit zu verlieren. B-Splines l"osen genau dieses Problem. Quadratische B-Splines sind st"uckweise quadratisch und haben einen minimalen Tr"ager. Sie quadratischen B-Splines formen eine Basis aller m"oglichen Funktionen $\sigma(t)$, wenn die St"utzpunkte $t_0,\dotsc, t_N$ an denen $\sigma(t)$ nicht quadratisch zu sein braucht, vorgegeben sind. 

F"ur "aquidistante St"utzstellen lassen sich die quadratischen B-Splines besonders einfach aufschreiben. F"ur die St"utzstellenmenge $\{0,1,2,3\}$ hat der quadratische B-Spline die Form
$$ b(t)=\begin{cases}
\frac12t^2 & \text{f"ur $t\in[0,1)$} \\
\frac12t^2-\frac32(t-1)^2 & \text{f"ur $t\in[1,2)$} \\
\frac12t^2-\frac32(t-1)^2+\frac32(t-2)^2 & \text{f"ur $t\in[2,3)$} \\
0 & \text{sonst.}
\end{cases} $$
Es gilt $b(1)=b(2)=\frac12$. An allen anderen ganzzahligen Stellen verschwindet  die Funktion $b$. Die erste und zweite Ableitung von $b$ sind
$$ \dot b(t)=\begin{cases}
t & \text{f"ur $t\in[0,1)$} \\
-2(t-\frac32) & \text{f"ur $t\in[1,2)$} \\
t-3 & \text{f"ur $t\in[2,3)$} \\
0 & \text{sonst}
\end{cases} 
\qquad \text{und} \qquad
\ddot b(t)=\begin{cases}
1 & \text{f"ur $t\in(0,1)$} \\
-2 & \text{f"ur $t\in(1,2)$} \\
1 & \text{f"ur $t\in(2,3)$} \\
0 & \text{f"ur $t\in\R\setminus[0,3]$\,.}
\end{cases} $$
Alle anderen quadratischen B-Splines mit aufeinanderfolgenden ganzzahligen St"utzstellen ergeben sich durch Translation des gegebenen B-Splines. Alle stetig differenzierbaren Funktionen, die auf den Intervallen $(k,k+1)$ f"ur alle ganzzahligen $k$ quadratische Funktionen sind, lassen sich durch Linearkombinationen dieser beschriebenen B-Splines darstellen. Also kann man die Funktion $\sigma(t)$ schreiben als
$$ \sigma(t) = \sum_{k=-\infty}^{\infty}\sigma_kb(t-k) $$
wenn die Koeffizienten $\sigma_k$ geeignet gew"ahlt werden. An ganzzahligen Stellen $j$ gilt somit
\begin{align*}
\sigma(j) &= \frac{\sigma_{j+1}+\sigma_{j+2}}{2} \\
\dot\sigma(j) &= \sigma_{j+1}-\sigma_{j+2} \\
\lim_{t\nearrow j}\ddot\sigma(t) &= \sigma_{j+1}-2\sigma_{j+2}+\sigma_{j+3} \\
\lim_{t\searrow\,j}\ddot\sigma(t) &= \sigma_{j}-2\sigma_{j+1}+\sigma_{j+2}\,.
\end{align*}
Die Grenzwerte bezeichnen durch die Pfeilrichtung einen linksseitigen bzw. einen rechtsseitigen Limes. Die Nebenbedingungen 
$$ \lvert\ddot\sigma(t)\rvert \le (\Im\dot\sigma(t))^2 
\qquad \text{und} \qquad
\Im\dot\sigma(t)\ge0 $$
sind "uberall erf"ullt, falls sie an den ganzzahligen Stellen gelten (siehe Abschnitt~\ref{sec:2ndansatz} f"ur die Begr"undung). Die Bedingung mit der zweiten Ableitung muss im links- und rechtsseitigen Grenzwert gelten. In der Sprache der Koeffizienten $\sigma_k$ die Nebenbedingungen nach kleineren Umformungen "aquivalent zu 
\begin{gather*}
\lvert\sigma_j-2\sigma_{j+1}+\sigma_{j+2}\rvert 
\le \max\{(\Im\sigma_j-\Im\sigma_{j+1})^2, 
    (\Im\sigma_{j+1}-\Im\sigma_{j+2})^2\} \\
\text{und} \qquad
\Im\sigma_j-\Im\sigma_{j+1}
\ge 0
\end{gather*}
f"ur alle ganzzahligen $j$. Mit 
$$ \tau_j := \sigma_j-\sigma_{j+1} $$
l"asst sich das noch k"urzer schreiben als
$$ \lvert\tau_j-\tau_{j+1}\rvert \le \max\{(\Im\tau_j)^2, (\Im\tau_{j+1})^2\}
\qquad \text{und} \qquad
\Im\tau_j \ge 0\,. $$
Die Nebenbedingunen lassen sich also auch mit diesem zweiten Ansatz leicht "uberpr"ufen. Im n"achsten Abschnitt soll die eigentliche Zielfunktion, also eine Norm der Restfunktion, genauer beschrieben werden.


\section{Norm f"ur die Restfunktion} \label{sec:norms4r}

Die Aufgabenstellung ist es, zu einer gegebenen hinreichend glatten Funktion $f:[a,b]\to\R$, Funktionen $a_i(t)$ und $\pphi_i(t)$, $i=1,\dotsc,N$ zu finden, so dass 
$$ r(t) := f(t)-\sum_{i=1}^N a_i(t)\cos\pphi_i(t) $$
m"oglichst klein wird. Bis jetzt haben wir noch nicht genau festgelegt, was wir unter \glqq{}klein\grqq{} verstehen. Das wollen wir nun nachholen, indem wir geeignete Normen f"ur die Restfunktion $r:[a,b]\to\R$ definieren. Folgende Ans"atze liegen nahe:
\begin{itemize}
\item $L_p$-Normen mit $1\le p\le\infty$, insbesondere sind die F"alle $p=1,2,\infty$ naheliegend. 
\item Sobolev-Normen, also Normen der Form
$$ \norm{r}_{W^{k,p}} := \left(\,\sum_{d=0}^k\int_a^b \lvert r^{(d)}(t)\rvert^p\,\de t\right)^{1/p}. $$
\end{itemize}
In der Praxis liegt $f$ oft nicht als Funktion vor, sondern als eine Folge von Messwerten. Um dann mit den $L_p$- oder Sobolev-Normen immernoch arbeiten zu k"onnen, wird $f$ dann typischerweise interpoliert. Die Berechnung der Integrale ist dann wahrscheinlich nicht mehr analytisch m"oglich, sondern nur noch numerisch, was die Berechnung der Norm sehr aufwendig machen kann. Dies umgeht man, indem man f"ur dir Norm nur die gemessenen Funktionswerte benutzt, also etwa
$$ \norm{r}_{\ell_p} := \left(\,\sum_{j=1}^{N}\lvert r(j)\rvert^p\right)^{1/p}, $$
f"ur $1\le p<\infty$ und die entsprechende Supremumsnorm im Fall $p=\infty$. 


\section{Lokale Optimumsuche} \label{sec:gradients}

F"ur die lokale Suche eines Optimums k"onnen verschiedenste Algorithmen verwendet werden, die Ungleichungen als Nebenbedingungen zulassen. Man kann zum Beispiel ein einfaches Gradientenverfahren ansetzen. 

Damit n"ahert man sich sehr schnell einem lokalen Optimum. Es ist absehbar, dass bei unserem Problem sehr viele lokale Optima existieren. Die eigentliche Schwierigkeit besteht darin, ein m"oglichst gutes lokales Optimum zu finden. Daf"ur braucht man einen m"oglichst guten Startpunkt f"ur die Optimierung oder eine M"oglichkeit, einem lokalen Maximum wieder zu entkommen, oder beides. Beide Ans"atze werden in den Abschnitten \ref{sec:initialval} und~\ref{sec:jumps} er"ortert. 


\section{Suche nach einer guten Anfangsn"aherung} \label{sec:initialval}

Es gibt bereits verschiedene Verfahren, ein Signal in verschiedene Frequenzen zu zerlegen. Dazu geh"ort die Hilbert-Huang-Transformation. Eine andere M"oglichkeit ist die Benutzung des Spektrograms der Short-Time-Fourier-Transformation oder das Wavelet-Energie-Spektrum bez"uglich des komplexwertigen Morlet-Wavelets. Aus diesen Diagrammen kann man recht gut die haupts"achlich vorkommenden Frequenzen zu verschiedenen Zeitpunkten ablesen. Dies k"onnte man als Grundlage f"ur eine Anfangsn"aherung nehmen. Eine Schwierigkeit, die dabei wahrscheinlich auftreten wird, ist die Verifizierung der Nebenbedingungen. Was tut man mit einer Anfangsn"aherung, die die Nebenbedingungen nicht erf"ullt?


\section{Phasenboost und genetische Algorithmen} \label{sec:jumps}

Viele lokale Maxima werden vermutlich durch lokale "Anderungen der Funktionen $a_i(t)$ und $\pphi_i(t)$ voneinander getrennt. Addiert man zu einer Phasenfunktion $\pphi_i(t)$ eine Funktion, die f"ur gen"ugend kleines $t$ konstant $0$ ist und ab einem gewissen $t$ konstant $2\pi$ ist und zwischen diesen konstanten Teilen verbunden wird, dann findet man bei $a_i(t)\cos\pphi_i(t)$ nur eine lokale "Anderung vor, n"amlich zwischen den beiden konstanten Abschnitten der addierten Funktion. Solch eine Addition bezeichnen wir als Phasenboost. Es ist vorstellbar, dass durch Phasenboosts viele Gr"aben zwischen lokalen Maxima "uberwunden werden k"onnen. 

Weiterhin ist es vermutlich sinnvoll, nicht nur lokal zu suchen, sondern mehrere lokale Optima gleichzeitig zu suchen. Gute L"osungen k"onnten dann wiederum kombiniert werden, um noch bessere L"osungen zu erhalten. Solche Algorithmen werden auch als genetische Algorithmen bezeichnet. Genetische Algorithmen umfassen folgende Elemente
\begin{itemize}
  \item {\em Selektion} zweier Individuen gem"a"s der Fitness in der Population. Als Fitness kann beispielsweise die Norm der Fehlerfunktion $r(t)$ angenommen werden.  
  \item {\em Kombination} (Kreuzung) der beiden selektierten Individuen zu einem neuen Individuum. Die Funktionen k"onnen etwa zerst"uckelt aneinander gesetzt werden.
  \item {\em Mutation} des Individuums. Dies kann zum einen bedeuten, dass die Funktion so mit einem Gradientenverfahren optimiert wird, dass man sich nahe einem lokalen Optimum befindet. Andererseits k"onnten auch Phasenboosts mit einflie"sen. 
\end{itemize}
Genetische Algorithmen bringen zudem noch eine Vielzahl an anderen Ideen und Methoden mit sich, die verwendet werden k"onnten. 


\end{document}
